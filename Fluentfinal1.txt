#!/bin/bash

# post_job_simple.sh - Simple version for direct runner usage

set -e

# Configuration
LOG_DIR="/usr/bin/actions-runner/_diag"
LOG_GROUP="githubrunnerlogs"
REGION="us-east-2"
HOSTNAME=$(hostname)

# Get RUN_ID from environment (most reliable sources)
if [ -n "$GITHUB_RUN_ID" ]; then
    RUN_ID="$GITHUB_RUN_ID"
elif [ -n "$RUNNER_TRACKING_ID" ]; then
    RUN_ID=$(echo "$RUNNER_TRACKING_ID" | grep -o '[0-9]\+' | head -1)
else
    echo "Error: No RUN_ID found in environment"
    echo "Available env vars:"
    env | grep -E "(GITHUB|RUNNER)" || echo "None found"
    exit 1
fi

echo "Streaming logs for Run ID: $RUN_ID"

# Find latest log file
LATEST_LOG=$(find "$LOG_DIR" -name "Worker_*.log" -type f -printf '%T@ %p\n' | sort -nr | head -1 | cut -d' ' -f2-)

if [ -z "$LATEST_LOG" ]; then
    echo "No log files found"
    exit 1
fi

LOG_STREAM="runner-${RUN_ID}-${HOSTNAME}"

# Create log stream
aws logs create-log-stream \
    --log-group-name "$LOG_GROUP" \
    --log-stream-name "$LOG_STREAM" \
    --region "$REGION" 2>/dev/null || true

# Stream the log file
echo "Streaming $LATEST_LOG to $LOG_STREAM"
aws logs put-log-events \
    --log-group-name "$LOG_GROUP" \
    --log-stream-name "$LOG_STREAM" \
    --log-events file://<(while IFS= read -r line; do echo "{\"timestamp\": $(date +%s%3N), \"message\": \"$line\"}"; done < "$LATEST_LOG") \
    --region "$REGION"

echo "Log streaming complete!"














--------------

#!/bin/bash

# post_job_enhanced.sh - Handles CloudWatch limits properly

set -e

# Configuration
LOG_DIR="/usr/bin/actions-runner/_diag"
LOG_GROUP="githubrunnerlogs"
REGION="us-east-2"
HOSTNAME=$(hostname)

# CloudWatch Limits
MAX_BATCH_SIZE=10000  # Max events per batch
MAX_BATCH_BYTES=1048576  # 1MB in bytes
BATCH_DELAY=0.2  # Delay between batches (seconds)

# Get RUN_ID from environment
if [ -n "$GITHUB_RUN_ID" ]; then
    RUN_ID="$GITHUB_RUN_ID"
else
    echo "Error: GITHUB_RUN_ID not found"
    exit 1
fi

echo "Streaming logs for Run ID: $RUN_ID"

# Find latest log file
LATEST_LOG=$(find "$LOG_DIR" -name "Worker_*.log" -type f -printf '%T@ %p\n' | sort -nr | head -1 | cut -d' ' -f2-)

if [ -z "$LATEST_LOG" ]; then
    echo "No log files found"
    exit 1
fi

LOG_STREAM="runner-${RUN_ID}-${HOSTNAME}"

# Create log stream
aws logs create-log-stream \
    --log-group-name "$LOG_GROUP" \
    --log-stream-name "$LOG_STREAM" \
    --region "$REGION" 2>/dev/null || true

echo "Streaming $LATEST_LOG to $LOG_STREAM"

# Function to send logs in batches respecting limits
stream_logs_with_limits() {
    local log_file="$1"
    local sequence_token=""
    local batch_count=0
    local total_lines=0
    local batch_lines=()
    local batch_size=0
    
    while IFS= read -r line; do
        if [ -n "$line" ]; then
            # Calculate line size (rough estimate)
            line_size=${#line}
            
            # Check if adding this line would exceed limits
            if [ ${#batch_lines[@]} -ge $MAX_BATCH_SIZE ] || [ $((batch_size + line_size)) -gt $MAX_BATCH_BYTES ]; then
                # Send current batch
                send_batch "${batch_lines[@]}"
                batch_lines=()
                batch_size=0
                sleep $BATCH_DELAY
            fi
            
            # Add line to batch
            batch_lines+=("$line")
            batch_size=$((batch_size + line_size))
            total_lines=$((total_lines + 1))
        fi
    done < "$log_file"
    
    # Send any remaining lines
    if [ ${#batch_lines[@]} -gt 0 ]; then
        send_batch "${batch_lines[@]}"
    fi
    
    echo "‚úÖ Sent $total_lines total lines in $batch_count batches"
}

# Function to send a batch of logs
send_batch() {
    local batch_lines=("$@")
    local batch_events="["
    local first=true
    
    for line in "${batch_lines[@]}"; do
        if [ "$first" = true ]; then
            first=false
        else
            batch_events+=","
        fi
        
        # Escape JSON and create event
        ESCAPED_LINE=$(echo "$line" | sed 's/"/\\"/g' | sed 's/\\/\\\\/g')
        TIMESTAMP=$(date +%s%3N)
        batch_events+="{\"timestamp\": $TIMESTAMP, \"message\": \"$ESCAPED_LINE\"}"
    done
    
    batch_events+="]"
    
    # Send to CloudWatch
    if [ -z "$sequence_token" ]; then
        RESPONSE=$(aws logs put-log-events \
            --log-group-name "$LOG_GROUP" \
            --log-stream-name "$LOG_STREAM" \
            --log-events "$batch_events" \
            --region "$REGION" 2>/dev/null || echo "{}")
    else
        RESPONSE=$(aws logs put-log-events \
            --log-group-name "$LOG_GROUP" \
            --log-stream-name "$LOG_STREAM" \
            --log-events "$batch_events" \
            --sequence-token "$sequence_token" \
            --region "$REGION" 2>/dev/null || echo "{}")
    fi
    
    # Update sequence token
    sequence_token=$(echo "$RESPONSE" | grep -o '"nextSequenceToken":"[^"]*"' | cut -d'"' -f4)
    
    batch_count=$((batch_count + 1))
    echo "  Batch $batch_count: sent ${#batch_lines[@]} lines"
}

# Stream the logs with proper limits
stream_logs_with_limits "$LATEST_LOG"

echo "Log streaming complete!"
























local file_states = {}

function extract_run_id_from_log(tag, timestamp, record)
    print("=== LUA FILTER START ===")
    print("LUA: Input tag: " .. tostring(tag))
    print("LUA: Input timestamp: " .. tostring(timestamp))
    
    if type(record) ~= "table" then
        print("LUA: ‚ùå ERROR: Record is not a table")
        return 2, timestamp, record
    end

    -- Debug: Show all record fields
    print("LUA: üìã Record fields:")
    for k, v in pairs(record) do
        print("LUA:   " .. k .. " = '" .. tostring(v) .. "'")
    end

    local filename = "default"
    if record.file_path then
        filename = tostring(record.file_path):match("([^/]+)$") or "unknown"
    end
    print("LUA: üìÅ Processing file: '" .. filename .. "'")

    if not file_states[filename] then
        file_states[filename] = {
            run_id = nil,
            found = false,
            expect_value = false
        }
        print("LUA: üÜï Initialized new state for file: " .. filename)
    end

    local state = file_states[filename]
    print("LUA: üìä Current state - run_id: '" .. tostring(state.run_id) .. "', found: " .. tostring(state.found) .. ", expect_value: " .. tostring(state.expect_value))

    -- If we found run_id previously, change the tag
    if state.found and state.run_id then
        local new_tag = "github.runner." .. state.run_id
        print("LUA: üè∑Ô∏è  Changing tag from '" .. tag .. "' to '" .. new_tag .. "'")
        print("LUA: ‚úÖ RETURN: 1, " .. new_tag .. ", timestamp, record")
        return 1, new_tag, timestamp, record
    end

    local content = record.log or ""
    content = tostring(content)
    print("LUA: üìÑ Content: '" .. content .. "'")

    if state.expect_value then
        print("LUA: üîç State: expecting_value = TRUE - Looking for v pattern")
        local run_id = content:match('%s*"v"%s*:%s*"([^"]+)"')
        print("LUA: üéØ Pattern match result: '" .. tostring(run_id) .. "'")
        
        if run_id then
            run_id = run_id:gsub("^%s+", ""):gsub("%s+$", "")
            state.run_id = run_id
            state.found = true
            state.expect_value = false
            
            local new_tag = "github.runner." .. run_id
            print("LUA: üéâ SUCCESS: Extracted run_id '" .. run_id .. "', changing tag to '" .. new_tag .. "'")
            print("LUA: ‚úÖ RETURN: 1, " .. new_tag .. ", timestamp, record")
            return 1, new_tag, timestamp, record
        else
            print("LUA: ‚ùå No v value found, resetting expect_value")
            state.expect_value = false
        end
    else
        print("LUA: üîç State: expecting_value = FALSE - Looking for k:run_id pattern")
        local has_run_id_key = content:find('%s*"k"%s*:%s*"run_id"')
        print("LUA: üéØ Pattern match result: " .. tostring(has_run_id_key))
        
        if has_run_id_key then
            state.expect_value = true
            print("LUA: ‚úÖ Found k:run_id, now expecting v value")
        else
            print("LUA: ‚û°Ô∏è No k:run_id found")
        end
    end

    print("LUA: üîÑ Keeping original tag: " .. tag)
    print("LUA: ‚úÖ RETURN: 2, timestamp, record")
    print("=== LUA FILTER END ===")
    return 2, timestamp, record
end

[SERVICE]
    Flush         1
    Log_Level     debug  # Change to debug for more details
    Daemon        off

[INPUT]
    Name              tail
    Path              /usr/bin/actions-runner/_diag/Worker_*.log
    Tag               github.runner
    Refresh_Interval  5
    Read_from_Head    false
    Skip_Long_Lines   on
    Path_Key          file_path
    # Add these for better debugging
    Buffer_Chunk_Size 1k
    Buffer_Max_Size   8k

[FILTER]
    Name                lua
    Match               github.runner
    Script              /etc/fluent-bit/extract_run_id.lua
    Call                extract_run_id_from_log

# Add stdout output to see what's happening
[OUTPUT]
    Name                stdout
    Match               *
    Format              json

[OUTPUT]
    Name                cloudwatch_logs
    Match               github.runner.*
    region              us-east-2
    log_group_name      githubrunnerlogs
    log_stream_name     runner-${TAG[2]}-${HOSTNAME}
    auto_create_group   true
    auto_create_stream  true
    log_retention_days  7

[OUTPUT]
    Name                cloudwatch_logs
    Match               github.runner
    region              us-east-2
    log_group_name      githubrunnerlogs
    log_stream_name     runner-unknown-${HOSTNAME}
    auto_create_group   true
    auto_create_stream  true
    log_retention_days  7
